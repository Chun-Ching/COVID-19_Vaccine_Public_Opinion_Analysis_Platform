{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 796/796 [00:01<00:00, 573.25it/s] \n",
      "Inference: 100%|██████████| 7/7 [15:07<00:00, 129.61s/it]\n",
      "Tokenization: 100%|██████████| 796/796 [00:00<00:00, 823.08it/s] \n",
      "Inference: 100%|██████████| 141/141 [4:10:21<00:00, 106.53s/it]  \n",
      "Tokenization: 100%|██████████| 796/796 [00:01<00:00, 467.51it/s]\n",
      "Inference: 100%|██████████| 7/7 [15:19<00:00, 131.34s/it]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['category', 'title', 'content', 'link'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:77\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Chun-Ching\\anaconda3\\envs\\ai23\\lib\\site-packages\\pandas\\core\\frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3509\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3510\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3511\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3513\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3514\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Chun-Ching\\anaconda3\\envs\\ai23\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5779\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5782\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   5784\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   5785\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5786\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chun-Ching\\anaconda3\\envs\\ai23\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5845\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5842\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   5844\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 5845\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['category', 'title', 'content', 'link'] not in index\""
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from collections import Counter\n",
    "from snownlp import SnowNLP\n",
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
    "\n",
    "df = pd.read_csv('category_vaccine.csv', sep='|')\n",
    "\n",
    "# ckiplab word segment (中研院斷詞)\n",
    "ws = CkipWordSegmenter(level=2)\n",
    "pos = CkipPosTagger(level=2)\n",
    "ner = CkipNerChunker(level=2)\n",
    "\n",
    "\n",
    "## Word Segmentation\n",
    "tokens = ws(df.contents)\n",
    "\n",
    "## POS\n",
    "tokens_pos = pos(tokens)\n",
    "\n",
    "## word pos pair 詞性關鍵字\n",
    "word_pos_pair = [list(zip(w, p)) for w, p in zip(tokens, tokens_pos)]\n",
    "\n",
    "## NER命名實體辨識\n",
    "entity_list = ner(df.contents)\n",
    "\n",
    "# Remove stop words and filter using POS tag (tokens_v2)\n",
    "#with open('stops_chinese_traditional.txt', 'r', encoding='utf8') as f:\n",
    "#    stops = f.read().split('\\n')\n",
    "\n",
    "# 過濾條件:兩個字以上 特定的詞性\n",
    "allowPOS = ['Na', 'Nb', 'Nc', 'VA', 'VAC', 'VB', 'VC']\n",
    "\n",
    "tokens_v2 = []\n",
    "for wp in word_pos_pair:\n",
    "    tokens_v2.append([w for w, p in wp if (len(w) >= 2) and p in allowPOS])\n",
    "\n",
    "# Insert tokens into dataframe (新增斷詞資料欄位)\n",
    "df['tokens'] = tokens\n",
    "df['tokens_v2'] = tokens_v2\n",
    "df['entities'] = entity_list\n",
    "df['token_pos'] = word_pos_pair\n",
    "\n",
    "# Calculate word count (frequency) 計算字頻(次數)\n",
    "# allowPOS 過濾條件: 特定的詞性 兩個字以上\n",
    "allowPOS = ['Na', 'Nb', 'Nc', 'VC']\n",
    "\n",
    "def word_frequency(wp_pair):\n",
    "    filtered_words = []\n",
    "    for word, pos in wp_pair:\n",
    "        if (pos in allowPOS) & (len(word) >= 2):\n",
    "            filtered_words.append(word)\n",
    "        #print('%s %s' % (word, pos))\n",
    "    counter = Counter(filtered_words)\n",
    "    return counter.most_common(200)\n",
    "\n",
    "\n",
    "keyfreqs = []\n",
    "for wp in word_pos_pair:\n",
    "    topwords = word_frequency(wp)\n",
    "    keyfreqs.append(topwords)\n",
    "\n",
    "df['top_key_freq'] = keyfreqs\n",
    "\n",
    "# Abstract (summary) and sentimental score(摘要與情緒分數)\n",
    "summary = []\n",
    "sentiment = []\n",
    "for text in df.contents:\n",
    "    sn = SnowNLP(text)\n",
    "    summary.append(sn.summary())\n",
    "    sentiment.append(round(sn.sentiments, 2))\n",
    "\n",
    "df['summary'] = summary\n",
    "df['sentiment'] = sentiment\n",
    "\n",
    "# Rearrange the colmun order for readability\n",
    "df = df[[\n",
    "    'categories', 'titles', 'contents', 'sentimen', 'summary',\n",
    "    'top_key_freq', 'tokens', 'tokens_v2', 'entities', 'token_pos', 'urls',\n",
    "]]\n",
    "\n",
    "# Save data to disk\n",
    "df.to_csv('vaccine_preprocessed.csv', sep='|', index=False)\n",
    "\n",
    "## Read it out 讀出看看\n",
    "#df = pd.read_csv('cna_dataset_preprocessed.csv', sep='|')\n",
    "#df.head(1)\n",
    "\n",
    "print(\"process OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process OK!\n"
     ]
    }
   ],
   "source": [
    "# Rearrange the colmun order for readability\n",
    "df = df[[\n",
    "    'categories', 'titles', 'contents', 'sentiment', 'summary',\n",
    "    'top_key_freq', 'tokens', 'tokens_v2', 'entities', 'token_pos', 'urls',\n",
    "]]\n",
    "\n",
    "# Save data to disk\n",
    "df.to_csv('vaccine_preprocessed.csv', sep='|', index=False)\n",
    "\n",
    "## Read it out 讀出看看\n",
    "#df = pd.read_csv('cna_dataset_preprocessed.csv', sep='|')\n",
    "#df.head(1)\n",
    "\n",
    "print(\"process OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>titles</th>\n",
       "      <th>contents</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>summary</th>\n",
       "      <th>top_key_freq</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_v2</th>\n",
       "      <th>entities</th>\n",
       "      <th>token_pos</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AZ</td>\n",
       "      <td>澎湖確診數升 兒童疫苗接種率已逾7成</td>\n",
       "      <td>澎湖COVID-19（2019冠狀病毒疾病）確診數攀升，澎湖縣政府今天公布新增182例，還有...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['澎湖5至11歲兒童疫苗接種種率已逾7成', '澎湖縣政府今天公布新增182例確診案例',...</td>\n",
       "      <td>[('澎湖', 5), ('疫苗', 5), ('接種', 4), ('確診', 3), (...</td>\n",
       "      <td>['澎湖', 'COVID-19', '（', '2019', '冠狀', '病毒', '疾...</td>\n",
       "      <td>['澎湖', '冠狀', '病毒', '疾病', '確診數', '澎湖縣', '政府', '...</td>\n",
       "      <td>[NerToken(word='澎湖', ner='GPE', idx=(0, 2)), N...</td>\n",
       "      <td>[('澎湖', 'Nc'), ('COVID-19', 'FW'), ('（', 'PARE...</td>\n",
       "      <td>https://news.google.com/articles/CBMiMmh0dHBzO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  categories              titles  \\\n",
       "0         AZ  澎湖確診數升 兒童疫苗接種率已逾7成   \n",
       "\n",
       "                                            contents  sentiment  \\\n",
       "0  澎湖COVID-19（2019冠狀病毒疾病）確診數攀升，澎湖縣政府今天公布新增182例，還有...        0.0   \n",
       "\n",
       "                                             summary  \\\n",
       "0  ['澎湖5至11歲兒童疫苗接種種率已逾7成', '澎湖縣政府今天公布新增182例確診案例',...   \n",
       "\n",
       "                                        top_key_freq  \\\n",
       "0  [('澎湖', 5), ('疫苗', 5), ('接種', 4), ('確診', 3), (...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['澎湖', 'COVID-19', '（', '2019', '冠狀', '病毒', '疾...   \n",
       "\n",
       "                                           tokens_v2  \\\n",
       "0  ['澎湖', '冠狀', '病毒', '疾病', '確診數', '澎湖縣', '政府', '...   \n",
       "\n",
       "                                            entities  \\\n",
       "0  [NerToken(word='澎湖', ner='GPE', idx=(0, 2)), N...   \n",
       "\n",
       "                                           token_pos  \\\n",
       "0  [('澎湖', 'Nc'), ('COVID-19', 'FW'), ('（', 'PARE...   \n",
       "\n",
       "                                                urls  \n",
       "0  https://news.google.com/articles/CBMiMmh0dHBzO...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read it out 讀出看看\n",
    "df = pd.read_csv('vaccine_preprocessed.csv', sep='|')\n",
    "df.head(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ai23')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2524f329b5609d74e66f331aeb181c6a96d56433d7d7e2bf478ea5bf8e2b1439"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
